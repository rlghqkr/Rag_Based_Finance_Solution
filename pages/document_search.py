import os
import streamlit as st
import tiktoken
import tempfile, uuid, shutil, time
from loguru import logger
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.document_loaders import Docx2txtLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

def tiktoken_len(text: str) -> int:
    return len(tiktoken.get_encoding("cl100k_base").encode(text))

def load_documents(files):
    temp_dir = tempfile.mkdtemp(prefix="st_upload_")
    docs = []
    try:
        for f in files:
            path = os.path.join(temp_dir, f"{uuid.uuid4().hex}_{f.name}")
            with open(path, "wb") as fp:
                fp.write(f.getvalue()); time.sleep(0.01)
            low = path.lower()
            try:
                if low.endswith(".pdf"):
                    loader = PyMuPDFLoader(path)
                    loaded = loader.load()
                    if not loaded or not any(doc.page_content.strip() for doc in loaded):
                        st.error(f"'{f.name}': í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì´ë¯¸ì§€/ìŠ¤ìº” PDFì¼ ìˆ˜ ìˆìŒ)")
                        continue
                elif low.endswith(".docx"):
                    loader = Docx2txtLoader(path)
                    loaded = loader.load()
                elif low.endswith(".pptx"):
                    loader = UnstructuredPowerPointLoader(path)
                    loaded = loader.load()
                else:
                    st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {f.name}")
                    continue
                if not loaded:
                    st.error(f"ë¹ˆ ë¬¸ì„œì…ë‹ˆë‹¤: {f.name}")
                    continue
                if not all(hasattr(doc, 'page_content') for doc in loaded):
                    st.error(f"ì˜ëª»ëœ ë¬¸ì„œ êµ¬ì¡°: {f.name}")
                    continue
                docs.extend(loaded)
            except Exception as e:
                logger.error(f"ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {f.name} - {str(e)}")
                st.error(f"'{f.name}' ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue
    except Exception as e:
        logger.error("load_documents error: %s", e)
        st.error("íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)
    return docs

def chunk_documents(docs):
    if not docs:
        st.error("ë¶„ì„ ê°€ëŠ¥í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    try:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,           # ë” ì‘ì€ ì²­í¬
            chunk_overlap=100,        # ë” ë„“ì€ ì¤‘ì²©
            length_function=tiktoken_len,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        return splitter.split_documents(docs)
    except Exception as e:
        logger.error(f"ì²­í‚¹ ì‹¤íŒ¨: {str(e)}")
        st.error("í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨")
        return []


def make_vectorstore(chunks):
    if not chunks:
        st.error("ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vs = FAISS.from_documents(chunks, embeddings)
        if vs is None:
            st.error("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨")
        return vs
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        st.error("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨")
        return None

def make_document_chain(vs):
    if vs is None:
        return None
    memory = ConversationBufferMemory(memory_key="chat_history",return_messages=True,output_key="answer")
    try:
        retr   = vs.as_retriever(search_kwargs={"k":3})
    except Exception as e:
        logger.error(f"as_retriever ì‹¤íŒ¨: {str(e)}")
        st.error("ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    mv     = st.session_state.model_version
    if mv=="GEMINI":
        llm = ChatGoogleGenerativeAI(
            google_api_key=GEMINI_API_KEY, model=st.session_state.gemini_model,
            temperature=st.session_state.temperature, top_p=st.session_state.top_p,
            max_output_tokens=st.session_state.max_tokens,
            system_instruction="You are a helpful financial assistant."
        )
    else:
        model_name = "gpt-4" if mv=="GPT-4" else "gpt-3.5-turbo"
        llm = ChatOpenAI(
            openai_api_key=OPENAI_API_KEY, model_name=model_name,
            temperature=st.session_state.temperature, top_p=st.session_state.top_p,
            frequency_penalty=st.session_state.frequency_penalty,
            presence_penalty=st.session_state.presence_penalty,
            max_tokens=st.session_state.max_tokens
        )
    try:
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm, retriever=retr, memory=memory,
            return_source_documents=True, verbose=False
        )
        return chain
    except Exception as e:
        logger.error(f"ëŒ€í™” ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        st.error("AI ì²´ì¸ ìƒì„± ì‹¤íŒ¨")
        return None

def render_document_search():
    st.header("ğŸ“„ ê¸ˆìœµ ë¬¸ì„œ ë¶„ì„")
    st.session_state.setdefault("doc_chain", None)
    st.session_state.setdefault("doc_msgs", [{"role":"assistant","content":"ë¬¸ì„œë¥¼ ì—…ë¡œë“œãƒ»ì²˜ë¦¬í•´ì£¼ì„¸ìš”."}])
    st.session_state.setdefault("doc_ready", False)

    uploaded = st.file_uploader(
        "PDF/DOCX/PPTX ì—…ë¡œë“œ", type=["pdf","docx","pptx"],
        accept_multiple_files=True, key="doc_upload"
    )

    if st.button("ë¬¸ì„œ ì²˜ë¦¬", disabled=not uploaded):
        with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
            docs  = load_documents(uploaded)
            if not docs:
                return
            chunks = chunk_documents(docs)
            if not chunks:
                return
            vs    = make_vectorstore(chunks)
            if vs is None:
                return
            chain = make_document_chain(vs)
            if chain is None:
                return
            st.session_state.doc_chain  = chain
            st.session_state.doc_ready  = True
            st.success(f"{len(docs)}ê°œ ë¬¸ì„œ, {len(chunks)}ê°œ ì²­í¬ ì¤€ë¹„ ì™„ë£Œ.")

    # ëŒ€í™” UI
    for m in st.session_state.doc_msgs:
        with st.chat_message(m["role"]):
            st.write(m["content"])

    if st.session_state.doc_ready:
        q = st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”â€¦")
        if q:
            st.session_state.doc_msgs.append({"role":"user","content":q})
            with st.chat_message("assistant"):
                try:
                    res = st.session_state.doc_chain({"question":q})
                    st.write(res["answer"])
                    for i,doc in enumerate(res.get("source_documents", []),1):
                        src  = doc.metadata.get("source","unknown")
                        page = doc.metadata.get("page","?")
                        st.markdown(f"> ì¶œì²˜{i}: (page{page})")
                    st.session_state.doc_msgs.append({"role":"assistant","content":res["answer"]})
                except Exception as e:
                    st.error(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
